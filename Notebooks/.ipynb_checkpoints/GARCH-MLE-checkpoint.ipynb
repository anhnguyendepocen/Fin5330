{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Brough Lecture Notes: GARCH Models - Estimation via MLE__\n",
    "\n",
    "<br>\n",
    "\n",
    "Finance 5330: Financial Econometrics <br>\n",
    "Tyler J. Brough <br>\n",
    "Last Updated: March 28, 2019 <br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "These notes are based in part on the excellent monograph [Introduction to Python for Econometrics, Statistics, and Data Analysis](https://www.kevinsheppard.com/images/b/b3/Python_introduction-2016.pdf) by the econometrician Kevin Sheppard of Oxford Univeristy. Many thanks to Dr. Sheppard for making his lecture material publically available. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating GARCH Models\n",
    "\n",
    "The standard procedure to fit GARCH models to historical returns time series data is to implement a numerical _maximum likelihood estimation_ (MLE) method. \n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ though [Bayesian](https://www.springer.com/us/book/9783540786566) methods have been shown to be superior!\n",
    "\n",
    "<br>\n",
    "\n",
    "The typical setup is as follows: \n",
    "\n",
    "* Continuously compounded returns are assumed to have a conditionally normal distribution $N(0, \\sigma_{t})$\n",
    "\n",
    "* We can estimate the GARCH parameter weights via a numerical optimization routine such as Nelder-Mead or Newton-Raphson.\n",
    "\n",
    "* That is, the numerical routine searches for the parameter values that maximizes the value of the likelihood function.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the normality assumption the probility density of $\\epsilon_{t}$, conditional on $\\sigma_{t}$, is \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "f(\\epsilon | \\sigma_{t}) = \\frac{1}{\\sqrt{2\\pi \\sigma_{t}}} e^{-0.5  \\frac{\\epsilon_{t}^{2}}{\\sigma_{t}}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Since the $\\epsilon_{t}$ are conditionally independent, the probability of observing the actual returns that are observed is  the product of the probabilities, this is given by the likelihood function:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\prod\\limits_{t=1}^{T} f(\\epsilon_{t} | \\sigma_{t}) = \\prod\\limits_{t=1}^{T} \\left( \\frac{1}{\\sqrt{2\\pi \\sigma_{t}}} e^{-0.5  \\frac{\\epsilon_{t}^{2}}{\\sigma_{t}}} \\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "For the GARCH(1,1) model, $\\sigma_{t}$ is a function of $\\omega$, $\\alpha$, and $\\beta$. The MLE will select values for these parameters $\\hat{\\omega}$, $\\hat{\\alpha}$, and $\\hat{\\beta}$ - that maximize the value of the probability of observing the returns we actually historically did observe. \n",
    "\n",
    "<br>\n",
    "\n",
    "Typically, it is easiest to maximize the value of the log-likelihood function as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\sum\\limits_{t=1}^{T} \\left[ -0.5 \\ln{(\\sigma_{t})} - 0.5 \\frac{\\epsilon_{t}^{2}}{\\sigma_{t}}\\right]\n",
    "$$\n",
    "\n",
    "We can omit the term $-0.5 \\ln{(2 \\pi)}$ since it does not affect the solution. Though sometimes it is left in. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this MLE estimation in Python by utilizing the [optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) module in [Scipy](https://docs.scipy.org/doc/scipy/reference/index.html), which contains a host of numerical optimization routines. \n",
    "\n",
    "<br>\n",
    "\n",
    "First, we need to define a function to implement the log-likelihood function.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "import seaborn\n",
    "from numpy import size, log, exp, pi, sum, diff, array, zeros, diag, mat, asarray, sqrt, copy\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from scipy.optimize import fmin_slsqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garch_likelihood(params, data, sigma2, out=None):\n",
    "    mu = params[0]\n",
    "    omega = params[1]\n",
    "    alpha = params[2]\n",
    "    beta = params[3]\n",
    "    \n",
    "    T = size(data, 0)\n",
    "    eps = data - mu\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        sigma2[t] = omega + alpha * eps[t-1]**2 + beta * sigma2[t-1]\n",
    "        \n",
    "    lls = 0.5 * (log(2 * pi) + log(sigma2) + eps**2/sigma2)\n",
    "    ll = sum(lls)\n",
    "    \n",
    "    if out is None:\n",
    "        results = ll\n",
    "    else:\n",
    "        results = (ll, lls, copy(sigma2))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We should begin with data simulated from the model as a first-run test case. So, let's import the simulation code.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set GARCH parameters (these might come from an estimated model)\n",
    "mu = log(1.15) / 252\n",
    "w = 10.0**-6\n",
    "a = 0.085\n",
    "b = 0.905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_garch(parameters, numObs):\n",
    "    \n",
    "    ## extract the parameter values\n",
    "    mu = parameters[0]\n",
    "    w = parameters[1]\n",
    "    a = parameters[2]\n",
    "    b = parameters[3]\n",
    "    \n",
    "    ## initialize arrays for storage\n",
    "    z = np.random.normal(size=(numObs + 1))\n",
    "    q = zeros((numObs + 1))\n",
    "    r = zeros((numObs + 1))\n",
    "    \n",
    "    ## fix initial values \n",
    "    q[0] = w / (1.0 - a - b)\n",
    "    r[0] = mu + z[0] * sqrt(q[0])\n",
    "    e = (r[0] - mu) \n",
    "    \n",
    "    ## run the main simulation loop\n",
    "    for t in range(1, numObs + 1):\n",
    "        q[t] = w + a * (e * e) + b * q[t-1]\n",
    "        r[t] = mu + z[t] * sqrt(q[t])\n",
    "        e = (r[t] - mu) \n",
    "        \n",
    "    ## return a tuple with both returns and conditional variances\n",
    "    return (r, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of trading days per year\n",
    "numObs = 252\n",
    "\n",
    "## daily continuously compounded rate of return correpsonding to 15% annual\n",
    "mu = log(1.15) / 252\n",
    "\n",
    "## drift and GARCH(1,1) parameters in an array\n",
    "params = array([mu, w, a, b])\n",
    "\n",
    "## run the simulation\n",
    "r, s = simulate_garch(params, numObs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Okay, now we will set up the estimation of the model on this simulated data. The goal is to see if the numerical search routine embedded in the maximum likelihood estimation\n",
    "can recapture the preset parameter weights given above. \n",
    "\n",
    "<br>\n",
    "\n",
    "It is important to test our software with a known scenario like this before we take the model to real-world data first as a sanity check. Otherwise we could end up chasing our tails for hours and hours without effect. \n",
    "\n",
    "<br>\n",
    "\n",
    "The numerical algorithm requires good starting values as initial conditions for the search to begin. Here we will follow Sheppard. (Notice that this feels \"prior\" like, as in the Bayesian sense) A more data-based way to do this is to use some kind of grid search to find values that have \"small\" log-likelihood values. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begVals = array([r.mean(), r.var() * .01, .09, .90])\n",
    "begVals = array([mu, w, a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: -893.5303730848248\n",
      "            Iterations: 7\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 3\n"
     ]
    }
   ],
   "source": [
    "finfo = np.finfo(np.float64)\n",
    "bounds = [(-10 * r.mean(), 10 * r.mean()), (finfo.eps, 2 * r.var()), (0.0, 1.0), (0.0, 1.0)]\n",
    "\n",
    "estimates = fmin_slsqp(garch_likelihood, begVals, bounds=bounds, args=(r,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.95995571e-04, 2.38208122e-06, 8.35140525e-02, 8.89178546e-01])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated mean return is:  0.00049600 vs  0.00055461\n",
      "The estimated omega value is:  0.00000238 vs  0.00000100\n",
      "The estimated alpha value is:  0.08351405 vs  0.08500000\n",
      "The estimated beta values is:  0.88917855 vs  0.90500000\n"
     ]
    }
   ],
   "source": [
    "print(f\"The estimated mean return is: {estimates[0] : 0.8f} vs {mu : 0.8f}\")\n",
    "print(f\"The estimated omega value is: {estimates[1] : 0.8f} vs {w : 0.8f}\")\n",
    "print(f\"The estimated alpha value is: {estimates[2] : 0.8f} vs {a : 0.08f}\")\n",
    "print(f\"The estimated beta values is: {estimates[3] : 0.8f} vs {b : 0.08f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___So much depends upon those initial guesses!___\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
