{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Brough Lecture Notes: GARCH Models - Estimation via MLE__\n",
    "\n",
    "<br>\n",
    "\n",
    "Finance 5330: Financial Econometrics <br>\n",
    "Tyler J. Brough <br>\n",
    "Last Updated: March 28, 2019 <br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "These notes are based in part on the excellent monograph [Introduction to Python for Econometrics, Statistics, and Data Analysis](https://www.kevinsheppard.com/images/b/b3/Python_introduction-2016.pdf) by the econometrician Kevin Sheppard of Oxford Univeristy. Many thanks to Dr. Sheppard for making his lecture material publically available. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating GARCH Models\n",
    "\n",
    "The standard procedure to fit GARCH models to historical returns time series data is to implement a numerical _maximum likelihood estimation_ (MLE) method. \n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ though [Bayesian](https://www.springer.com/us/book/9783540786566) methods have been shown to be superior!\n",
    "\n",
    "<br>\n",
    "\n",
    "The typical setup is as follows: \n",
    "\n",
    "* Continuously compounded returns are assumed to have a conditionally normal distribution $N(0, \\sigma_{t})$\n",
    "\n",
    "* We can estimate the GARCH parameter weights via a numerical optimization routine such as Nelder-Mead or Newton-Raphson.\n",
    "\n",
    "* That is, the numerical routine searches for the parameter values that maximizes the value of the likelihood function.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the normality assumption the probility density of $\\epsilon_{t}$, conditional on $\\sigma_{t}$, is \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "f(\\epsilon | \\sigma_{t}) = \\frac{1}{\\sqrt{2\\pi \\sigma_{t}}} e^{-0.5  \\frac{\\epsilon_{t}^{2}}{\\sigma_{t}}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Since the $\\epsilon_{t}$ are conditionally independent, the probability of observing the actual returns that are observed is  the product of the probabilities, this is given by the likelihood function:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\prod\\limits_{t=1}^{T} f(\\epsilon_{t} | \\sigma_{t}) = \\prod\\limits_{t=1}^{T} \\left( \\frac{1}{\\sqrt{2\\pi \\sigma_{t}}} e^{-0.5  \\frac{\\epsilon_{t}^{2}}{\\sigma_{t}}} \\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "For the GARCH(1,1) model, $\\sigma_{t}$ is a function of $\\omega$, $\\alpha$, and $\\beta$. The MLE will select values for these parameters $\\hat{\\omega}$, $\\hat{\\alpha}$, and $\\hat{\\beta}$ - that maximize the value of the probability of observing the returns we actually historically did observe. \n",
    "\n",
    "<br>\n",
    "\n",
    "Typically, it is easiest to maximize the value of the log-likelihood function as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\sum\\limits_{t=1}^{T} \\left[ -0.5 \\ln{(\\sigma_{t})} - 0.5 \\frac{\\epsilon_{t}^{2}}{\\sigma_{t}}\\right]\n",
    "$$\n",
    "\n",
    "We can omit the term $-0.5 \\ln{(2 \\pi)}$ since it does not affect the solution. Though sometimes it is left in. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this MLE estimation in Python by utilizing the [optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) module in [Scipy](https://docs.scipy.org/doc/scipy/reference/index.html), which contains a host of numerical optimization routines. \n",
    "\n",
    "<br>\n",
    "\n",
    "First, we need to define a function to implement the log-likelihood function.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "import seaborn\n",
    "from numpy import size, log, exp, pi, sum, diff, array, zeros, diag, mat, asarray, sqrt, copy\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from scipy.optimize import fmin_slsqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garch_likelihood(params, data, sigma2, out=None):\n",
    "    mu = params[0]\n",
    "    omega = params[1]\n",
    "    alpha = params[2]\n",
    "    beta = params[3]\n",
    "    \n",
    "    T = size(data, 0)\n",
    "    eps = data - mu\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        sigma2[t] = omega + alpha * eps[t-1]**2 + beta * sigma2[t-1]\n",
    "        \n",
    "    lls = 0.5 * (log(2 * pi) + log(sigma2) + eps**2/sigma2)\n",
    "    ll = sum(logliks)\n",
    "    \n",
    "    if out is None:\n",
    "        results = ll\n",
    "    else:\n",
    "        results = (ll, lls, copy(sigma2))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
